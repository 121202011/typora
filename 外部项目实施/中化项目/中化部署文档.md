# 中化薪酬管控分析系统部署文档

| 项目信息 | 详情                 |
| -------- | -------------------- |
| 项目名称 | 中化薪酬管控分析项目 |
| 版本号   | v1.0.0               |
| 部署环境 | 生产环境 / 测试环境  |
| 部署日期 | 2025-06-10           |
| 维护人员 | 中智运维团队         |

## 一、部署目标

### 1.1 实现功能

承托最新薪酬监管政策要求，围绕国资委国资监管采集数据、咨询设计分析模型需企业自定义导入数据开展看板呈现分析，满足集团层层向下管控和单体企业运营需求。
聚焦 “1+M+N” 个监管场景，针对关键监管业务和重点关注人群，通过主题驾驶舱形式开展企业管控落地的全景式穿透分析，实现全方位薪酬业务在线监管，深度挖掘数据消费价值，有效支撑薪酬管理决策。

### 1.2 部署范围

| 服务名称      | 主要端口           | 用途                     |
| ------------- | ------------------ | ------------------------ |
| Redis         | 6379               | 数据缓存                 |
| RabbitMQ      | 5672、15672、25672 | 消息队列                 |
| Elasticsearch | 9200               | 分布式搜索与数据分析     |
| Kibana        | 5601               | 数据可视化               |
| Nginx         | 80                 | 反向代理                 |
| Keepalived    | VRRP 协议 112      | 服务高可用               |
| MinIO         | 9000、9001         | 分布式存储               |
| FineReport    | 80、8080           | 业务报表                 |
| DM            | 5236               | 数据库                   |
| 前端          | -                  | 业务服务（用户交互界面） |
| 后端          | -                  | 业务服务（逻辑处理）     |

### 1.3 系统功能架构图

```plaintext
客户端 → 内网 → 负载均衡服务器（域名1、域名2、域名3）
                ↓
        ┌─────────────────────────────────────────┐
        │ 应用服务器 | 数据库服务器 | 中间件 | 文件服务器 │
        └─────────────────────────────────────────┘
```

### 1.4 产品部署方案

| 服务名称                    | 弹性云服务器名称              | 规格       | 新 IPv4 地址   | 根磁盘容量 | 数据盘容量 |
| --------------------------- | ----------------------------- | ---------- | -------------- | ---------- | ---------- |
| 测试环境 (FineReport)       | ecs-ZHRLYTHPTJSXM_XA-APP-0106 | 16 核 32GB | 10.172.131.197 | 50GB       | 500GB      |
| 测试环境 (数据库 + 中间件)  | ecs-ZHRLYTHPTJSXM_XA-APP-0107 | 16 核 32GB | 10.172.131.73  | 50GB       | 500GB      |
| 测试环境 (前端 + 后端)      | ecs-ZHRLYTHPTJSXM_XA-APP-0105 | 32 核 64GB | 10.172.131.142 | 50GB       | 300GB      |
| ElasticSearch 集群 - 节点 1 | ecs-ZHRLYTHPTJSXM_XA-APP-0101 | 8 核 16GB  | 10.172.131.68  | 50GB       | 200GB      |
| ElasticSearch 集群 - 节点 2 | ecs-ZHRLYTHPTJSXM_XA-APP-0100 | 8 核 16GB  | 10.172.131.201 | 50GB       | 200GB      |
| ElasticSearch 集群 - 节点 3 | ecs-ZHRLYTHPTJSXM_XA-APP-0102 | 8 核 16GB  | 10.172.131.206 | 50GB       | 200GB      |
| Redis 主从集群 - 节点 1     | ecs-ZHRLYTHPTJSXM_XA-APP-0094 | 4 核 8GB   | 10.172.131.145 | 50GB       | 100GB      |
| Redis 主从集群 - 节点 2     | ecs-ZHRLYTHPTJSXM_XA-APP-0091 | 4 核 8GB   | 10.172.131.138 | 50GB       | 100GB      |
| RabbitMQ 集群 - 节点 1      | ecs-ZHRLYTHPTJSXM_XA-APP-0093 | 4 核 8GB   | 10.172.131.111 | 50GB       | 100GB      |
| RabbitMQ 集群 - 节点 2      | ecs-ZHRLYTHPTJSXM_XA-APP-0092 | 4 核 8GB   | 10.172.131.113 | 50GB       | 100GB      |
| RabbitMQ 集群 - 节点 3      | ecs-ZHRLYTHPTJSXM_XA-APP-0095 | 4 核 8GB   | 10.172.131.186 | 50GB       | 100GB      |
| 负载均衡 1                  | ecs-ZHRLYTHPTJSXM_XA-APP-0086 | 4 核 8GB   | 10.172.131.66  | 50GB       | -          |
| 负载均衡 2                  | ecs-ZHRLYTHPTJSXM_XA-APP-0085 | 4 核 8GB   | 10.172.131.174 | 50GB       | -          |
| 后端服务 1                  | ecs-ZHRLYTHPTJSXM_XA-APP-0089 | 32 核 64GB | 10.172.131.164 | 50GB       | 200GB      |
| 后端服务 2                  | ecs-ZHRLYTHPTJSXM_XA-APP-0090 | 32 核 64GB | 10.172.131.98  | 50GB       | 200GB      |
| 前端服务 1                  | ecs-ZHRLYTHPTJSXM_XA-APP-0087 | 8 核 16GB  | 10.172.131.247 | 50GB       | 100GB      |
| 前端服务 2                  | ecs-ZHRLYTHPTJSXM_XA-APP-0088 | 8 核 16GB  | 10.172.131.58  | 50GB       | 100GB      |
| 数据库 - 节点 1             | ecs-ZHRLYTHPTJSXM_XA-APP-0099 | 32 核 64GB | 10.172.131.156 | 50GB       | 500GB      |
| 数据库 - 节点 2             | ecs-ZHRLYTHPTJSXM_XA-APP-0098 | 32 核 64GB | 10.172.131.242 | 50GB       | 500GB      |
| MinIO 对象存储集群 - 节点 1 | ecs-ZHRLYTHPTJSXM_XA-APP-0096 | 8 核 32GB  | 10.172.131.76  | 50GB       | 200GB      |
| MinIO 对象存储集群 - 节点 2 | ecs-ZHRLYTHPTJSXM_XA-APP-0097 | 8 核 32GB  | 10.172.131.24  | 50GB       | 200GB      |
| FineReport 组件 & 运维平台  | ecs-ZHRLYTHPTJSXM_XA-APP-0104 | 8 核 32GB  | 10.172.131.236 | 50GB       | 300GB      |
| FineReport 服务器           | ecs-ZHRLYTHPTJSXM_XA-APP-0103 | 16 核 64GB | 10.172.131.227 | 50GB       | 200GB      |

## 二、环境要求

| 组件           | 要求          | 检查命令                   |
| -------------- | ------------- | -------------------------- |
| 服务器         | 麒麟 x86 系统 | `cat /etc/*release*`       |
| Docker         | 版本 = 27.3.1 | `docker --version`         |
| Docker Compose | 版本 = 2.35.1 | `docker-compose --version` |

## 三、部署步骤

### 3.1 基础环境准备

#### 3.1.1 Docker 环境安装

1. **上传安装包**
   将 `docker-27.3.1-x86.tgz` 上传至 `/opt/` 目录。

2. **解压安装包**

   ```bash
   tar -xvf /opt/docker-27.3.1-x86.tgz
   ```

3. **移动 Docker 文件到系统可执行目录**

   ```bash
   mv docker/* /usr/local/bin/
   ```

4. **创建 Docker 配置文件目录**

   ```bash
   mkdir /etc/docker/
   ```

5. **添加 Docker 配置文件（daemon.json）**

   ```bash
   cat > /etc/docker/daemon.json <<EOF
   {
       "storage-driver": "overlay2",
       "max-concurrent-downloads": 20,
       "live-restore": true,
       "max-concurrent-uploads": 10,
       "debug": true,
       "log-opts": {
           "max-size": "100m",
           "max-file": "5"
       },
       "bip": "11.16.0.1/16"
   }
   EOF
   ```

6. **创建 Docker 服务文件（docker.service）**

   ```bash
   cat > /etc/systemd/system/docker.service <<EOF
   [Unit]
   Description=Docker Application Container Engine
   Documentation=https://docs.docker.com
   After=network-online.target firewalld.service
   Wants=network-online.target
   
   [Service]
   Type=notify
   ExecStart=/usr/local/bin/dockerd
   ExecReload=/bin/kill -s HUP
   LimitNOFILE=infinity
   LimitNPROC=infinity
   LimitCORE=infinity
   TimeoutStartSec=0
   Delegate=yes
   KillMode=process
   Restart=on-failure
   StartLimitBurst=3
   StartLimitInterval=60s
   
   [Install]
   WantedBy=multi-user.target
   EOF
   ```

7. **启动 Docker 服务并设置开机自启**

   ```bash
   systemctl enable --now docker
   ```

8. **验证 Docker 状态与版本**

   ```bash
   # 查看服务状态
   systemctl status docker
   # 查看版本
   docker --version 或 docker info
   ```

#### 3.1.2 Docker Compose 环境安装

1. **上传安装包**
   将 `docker-compose-linux_2.35.1-x86_64` 上传至 `/opt/` 目录。

2. **移动文件到系统可执行目录**

   ```bash
   mv /opt/docker-compose-linux_2.35.1-x86_64 /usr/local/bin/docker-compose
   ```

3. **添加执行权限**

   ```bash
   chmod +x /usr/local/bin/docker-compose
   ```

4. **验证安装**

   ```bash
   docker-compose --version
   ```

#### 3.1.3 防火墙与 SELinux 配置

1. **防火墙操作**

   ```bash
   # 查看防火墙状态
   systemctl status firewalld
   # 关闭防火墙
   systemctl stop firewalld
   # 禁用防火墙开机自启
   systemctl disable firewalld
   ```

   > 注：默认关闭防火墙；若需开启，需额外配置中间件端口的访问策略。

2. **SELinux 配置（设置为 disabled）**

   ```bash
   # 临时关闭
   setenforce 0
   # 永久关闭（编辑配置文件）
   vim /etc/selinux/config
   # 修改为：SELINUX=disabled
   ```

### 3.2 中间件安装

#### 3.2.1 Redis 安装部署（主从集群）

##### 1. 通用准备操作

1. **创建服务目录与存储目录**

   ```bash
   mkdir -p /opt/redis/data
   ```

2. **上传配置文件**
   将 `redis.conf` 和 `docker-compose.yml` 上传至 `/opt/redis/` 目录。

##### 2. 主节点配置（10.172.131.145）

1. **编写主节点 redis.conf**

   ```bash
   cat > /opt/redis/redis.conf <<EOF
   # 基础设置
   bind 0.0.0.0
   port 6379
   protected-mode no
   
   # 安全设置
   requirepass VHRredis@admin
   
   # 持久化设置
   save 900 1
   save 300 10
   save 60 10000
   dbfilename dump.rdb
   appendonly yes
   appendfilename "appendonly.aof"
   appendfsync everysec
   EOF
   ```

2. **编写 docker-compose.yml**

   ```bash
   cat > /opt/redis/docker-compose.yml <<EOF
   version: '3.8'
   services:
     redis:
       image: "harbor.ciicsh.com/library/redis:7.4.3-alpine"
       container_name: "redis"
       command: redis-server /usr/local/etc/redis/redis.conf
       volumes:
         - "/opt/redis/data:/data"
         - "/opt/redis/redis.conf:/usr/local/etc/redis/redis.conf"
       restart: always
   network_mode: "host"
   EOF
   ```

##### 3. 从节点配置（10.172.131.138）

1. **编写从节点 redis.conf（新增复制集配置）**

   ```bash
   cat > /opt/redis/redis.conf <<EOF
   # 基础设置（同主节点）
   bind 0.0.0.0
   port 6379
   protected-mode no
   
   # 安全设置（同主节点）
   requirepass VHRredis@admin
   
   # 持久化设置（同主节点）
   save 900 1
   save 300 10
   save 60 10000
   dbfilename dump.rdb
   appendonly yes
   appendfilename "appendonly.aof"
   appendfsync everysec
   
   # 复制集设置（指向主节点）
   replicaof 10.172.131.145 6379
   masterauth VHRredis@admin
   EOF
   ```

2. **docker-compose.yml 与主节点一致**

##### 4. 启动与验证

1. **导入镜像**

   ```bash
   docker load < redis.tar.gz
   ```

2. **启动服务（主、从节点均执行）**

   ```bash
   cd /opt/redis
   docker-compose -f docker-compose.yml up -d
   ```

3. **查看服务状态**

   ```bash
   docker-compose ps -a
   ```

4. **关闭服务（如需）**

   ```bash
   docker-compose -f docker-compose.yml down
   ```

#### 3.2.2 Elasticsearch 安装部署（3 节点集群）

##### 1. 通用准备操作

1. **上传文件**
   将 `docker-compose.yml`、`elasticsearch.yml`、`elasticsearch-8.17.6-x86.tar` 上传至 `/opt/elasticsearch/` 目录。

2. **创建服务目录与数据目录**

   ```bash
   mkdir -p /opt/elasticsearch/data
   ```

3. **修改主机名（每个节点分别执行）**

   ```bash
   # 节点1（10.172.131.68）
   hostnamectl set-hostname ecs-zhrlythptjsxm-xa-app-0101
   # 节点2（10.172.131.201）
   hostnamectl set-hostname ecs-zhrlythptjsxm-xa-app-0100
   # 节点3（10.172.131.206）
   hostnamectl set-hostname ecs-zhrlythptjsxm-xa-app-0102
   ```

4. **目录赋权**

   ```bash
   chown -R 1000:1000 /opt/elasticsearch/
   ```

5. **配置用户组资源限制**

   ```bash
   vim /etc/security/limits.conf
   # 添加以下内容
   * hard nofile 131072
   * soft nproc 2048
   * hard nproc 4096
   ```

6. **修改内核参数**

   ```bash
   vim /etc/sysctl.conf
   # 添加以下内容
   vm.swappiness=1
   vm.max_map_count=655360
   fs.file-max = 204800
   # 生效配置
   sysctl -p
   ```

##### 2. 配置文件编写（每个节点需修改部分参数）

1. **节点 1（10.172.131.68）elasticsearch.yml**

   ```bash
   cat > /opt/elasticsearch/elasticsearch.yml <<EOF
   # 节点基础配置
   cluster.name: vhrelastic
   node.name: ecs-zhrlythptjsxm-xa-app-0101
   network.host: 10.172.131.68
   http.port: 9200
   transport.port: 9300
   
   # 集群发现配置
   discovery.seed_hosts: ["10.172.131.68", "10.172.131.201", "10.172.131.206"]
   cluster.initial_master_nodes: ["ecs-zhrlythptjsxm-xa-app-0102"]
   
   # 跨站访问配置
   http.cors.enabled: true
   http.cors.allow-origin: "*"
   http.cors.allow-headers: Authorization
   
   # X-Pack安全配置
   xpack.security.enabled: true
   xpack.security.transport.ssl.enabled: true
   xpack.security.transport.ssl.verification_mode: certificate
   xpack.security.transport.ssl.keystore.path: /usr/share/elasticsearch/config/elastic-certificates.p12
   xpack.security.transport.ssl.truststore.path: /usr/share/elasticsearch/config/elastic-certificates.p12
   xpack.security.http.ssl.enabled: true
   xpack.security.http.ssl.verification_mode: certificate
   xpack.security.http.ssl.keystore.path: /usr/share/elasticsearch/config/elastic-certificates.p12
   xpack.security.http.ssl.keystore.password: ""
   xpack.security.http.ssl.truststore.path: /usr/share/elasticsearch/config/elastic-certificates.p12
   xpack.security.http.ssl.truststore.password: ""
   EOF
   ```

2. **节点 2/3 修改项**

   - 节点 2（10.172.131.201）：`node.name=ecs-zhrlythptjsxm-xa-app-0100`、`network.host=10.172.131.201`
   - 节点 3（10.172.131.206）：`node.name=ecs-zhrlythptjsxm-xa-app-0102`、`network.host=10.172.131.206`

##### 3. 生成 SSL 证书（仅在任一节点执行，如节点 1）

1. **启动临时容器生成证书**

   ```bash
   docker run -it --name elasticsearch-container -w /usr/share/elasticsearch/bin harbor.ciicsh.com/library/elasticsearch:8.17.6 /bin/bash
   ```

2. **生成 CA 根证书**

   ```bash
   ./elasticsearch-certutil ca --days 36500 --out ca.p12
   ```

3. **生成节点证书（包含所有节点 IP 和主机名）**

   ```bash
   ./elasticsearch-certutil cert \
   --ca /usr/share/elasticsearch/ca.p12 \
   --dns ecs-zhrlythptjsxm-xa-app-0101,ecs-zhrlythptjsxm-xa-app-0100,ecs-zhrlythptjsxm-xa-app-0102 \
   --ip 10.172.131.68,10.172.131.201,10.172.131.206 \
   --days 36500 \
   --out /usr/share/elasticsearch/elastic-certificates.p12 \
   --pass ""
   ```

4. **复制证书到本地目录**

   ```bash
   docker cp elasticsearch-container:/usr/share/elasticsearch/ca.p12 /opt/elasticsearch
   docker cp elasticsearch-container:/usr/share/elasticsearch/elastic-certificates.p12 /opt/elasticsearch/
   ```

5. **证书赋权**

   ```bash
   chmod +rx /opt/elasticsearch/elastic-certificates.p12
   chmod +rx /opt/elasticsearch/ca.p12
   ```

6. **复制证书到其他节点**
   将 `ca.p12` 和 `elastic-certificates.p12` 复制到节点 2、3 的 `/opt/elasticsearch/` 目录。

##### 4. 启动与验证

1. **导入镜像（所有节点执行）**

   ```bash
   docker load < /opt/elasticsearch/elasticsearch-8.17.6-x86.tar
   ```

2. **启动服务（所有节点执行）**

   ```bash
   cd /opt/elasticsearch/
   docker-compose -f docker-compose.yml up -d
   ```

3. **添加 Kibana 账号（任一节点执行）**

   ```bash
   docker exec -it es /bin/bash
   bin/elasticsearch-reset-password -u kibana_system -i
   ```

4. **查看集群状态**

   ```bash
   # 查看节点列表
   curl --basic -u elastic:VHRelastic@admin -k https://10.172.131.206:9200/_cat/nodes?v
   # 查看集群健康状态
   curl --basic -u elastic:VHRelastic@admin -k https://10.172.131.206:9200/_cat/health?v
   ```

#### 3.2.3 Kibana 安装部署

##### 1. 准备操作

1. **上传文件**
   将 `docker-compose.yml`、`kibana.tar.gz`、`kibana.yml` 上传至 `/opt/kibana/` 目录。

2. **创建数据目录并赋权**

   ```bash
   mkdir -p /opt/kibana/data
   chown -R 1000:1000 /opt/kibana/
   ```

##### 2. 配置文件编写

1. **kibana.yml**

   ```bash
   cat > /opt/kibana/kibana.yml <<EOF
   server.port: 5601
   server.host: "0.0.0.0"
   elasticsearch.hosts: ["https://10.172.131.206:9200","https://10.172.131.68:9200","https://10.172.131.201:9200"]
   i18n.locale: "zh-CN"
   elasticsearch.requestTimeout: 90000
   elasticsearch.username: "kibana_system"
   elasticsearch.password: "VHRkibana@admin"
   elasticsearch.ssl.verificationMode: none
   EOF
   ```

2. **docker-compose.yml**

   ```bash
   cat > /opt/kibana/docker-compose.yml <<EOF
   version: '3.8'
   services:
     kibana:
       container_name: kibana
       image: "harbor.ciicsh.com/library/kibana:8.17.6"
       environment:
         TZ: 'Asia/Shanghai'
       volumes:
         - /opt/kibana/kibana.yml:/usr/share/kibana/config/kibana.yml
         - /opt/kibana/data:/usr/share/kibana/data
         - /opt/elasticsearch/elastic-stack-ca.p12:/usr/share/kibana/config/elastic-stack-ca.p12
       ports:
         - 5601:5601
       restart: always
   network_mode: "host"
   EOF
   ```

##### 3. 启动与验证

1. **导入镜像**

   ```bash
   docker load < /opt/kibana/kibana.tar.gz
   ```

2. **启动服务**

   ```bash
   cd /opt/kibana/
   docker-compose -f docker-compose.yml up -d
   ```

3. **查看服务状态**

   ```bash
   docker-compose ps -a
   ```

#### 3.2.4 RabbitMQ 安装部署（3 节点集群）

##### 1. 通用准备操作

1. **上传文件**
   将 `docker-compose.yml`、`rabbitmq-management_3.13.7-amd64.tar.gz`、`rabbitmq_delayed_message_exchange-3.13.0.ez` 上传至 `/opt/rabbitmq/` 目录。

2. **配置主机 hosts（所有节点执行）**

   ```bash
   vim /etc/hosts
   # 添加以下内容（替换为实际IP和主机名）
   10.172.131.111  ecs-zhrlythptjsxm-xa-app-0093
   10.172.131.113  ecs-zhrlythptjsxm-xa-app-0092
   10.172.131.186  ecs-zhrlythptjsxm-xa-app-0095
   ```

3. **创建目录并赋权**

   ```bash
   mkdir -p /opt/rabbitmq/{data,plugins}
   chown -R 1000:1000 /opt/rabbitmq
   ```

4. **复制插件到插件目录**

   ```bash
   cp /opt/rabbitmq/rabbitmq_delayed_message_exchange-3.13.0.ez /opt/rabbitmq/plugins/
   ```

##### 2. 配置文件编写（docker-compose.yml）

```bash
cat > /opt/rabbitmq/docker-compose.yml <<EOF
services:
  rabbitmq:
    container_name: rabbitmq
    image: harbor.ciicsh.com/base/rabbitmq:3.13.7-management
    privileged: true
    network_mode: host
    restart: always
    volumes:
      - /opt/rabbitmq/data:/var/lib/rabbitmq
      - /opt/rabbitmq/plugins/rabbitmq_delayed_message_exchange-3.13.0.ez:/plugins/rabbitmq_delayed_message_exchange-3.13.0.ez
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=VHRrabbitmq@admin
EOF
```

##### 3. 启动与集群配置

1. **导入镜像（所有节点执行）**

   ```bash
   docker load < /opt/rabbitmq/rabbitmq-management_3.13.7-amd64.tar.gz
   ```

2. **启动服务（所有节点执行）**

   ```bash
   cd /opt/rabbitmq
   docker-compose -f docker-compose.yml up -d
   ```

3. **配置集群（从节点执行，如 113、186 节点）**

   ```bash
   # 进入容器
   docker exec -it rabbitmq /bin/bash
   # 停止app服务
   rabbitmqctl stop_app
   # 重置节点
   rabbitmqctl reset
   # 加入集群（node_name格式：rabbit@主机名，如rabbit@ecs-zhrlythptjsxm-xa-app-0093）
   rabbitmqctl join_cluster rabbit@ecs-zhrlythptjsxm-xa-app-0093
   # 启动app服务
   rabbitmqctl start_app
   # 查看集群状态
   rabbitmqctl cluster_status
   ```

4. **创建业务账号与权限配置（任一节点执行）**

   ```bash
   # 进入容器
   docker exec -it rabbitmq /bin/bash
   # 1. 创建用户vhr
   rabbitmqctl add_user vhr VHRrabbitmq@admin
   # 2. 设置为管理员（可选）
   rabbitmqctl set_user_tags vhr administrator
   # 3. 创建虚拟主机vhr
   rabbitmqctl add_vhost vhr
   # 4. 分配vhost全部权限
   rabbitmqctl set_permissions -p vhr vhr ".*" ".*" ".*"
   # 5. 添加高可用策略
   rabbitmqctl set_policy -p vhr ha-mode ".*" '{"ha-mode":"all","ha-sync-mode":"automatic"}' --priority 0 --apply-to all
   # 6. 启用延迟消息插件
   rabbitmq-plugins enable rabbitmq_delayed_message_exchange-3.13.0.ez
   ```

5. **验证服务状态**

   ```bash
   docker-compose ps -a
   ```

#### 3.2.5 Nginx 安装部署

##### 1. 准备操作

1. **上传文件**
   将 `docker-compose.yml`、`nginx.conf`、`nginx_1.22.1-alpine-x86.tar.gz`、`default.conf` 上传至 `/opt/nginx/` 目录。

2. **创建目录**

   ```bash
   mkdir -p /opt/nginx/conf.d /opt/nginx/logs
   ```

##### 2. 配置文件编写

1. **主配置文件（nginx.conf）**

   ```bash
   cat > /opt/nginx/nginx.conf <<EOF
   user  nginx;
   worker_processes  auto;
   
   error_log  /var/log/nginx/error.log warn;
   pid        /var/run/nginx.pid;
   
   events {
       worker_connections  5000;
   }
   
   http {
       include       /etc/nginx/mime.types;
       default_type  application/octet-stream;
   
       # 日志格式（普通格式）
       log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
                         '$status $body_bytes_sent "$http_referer" '
                         '"$http_user_agent" "$http_x_forwarded_for"';
   
       # 日志格式（JSON格式）
       log_format access_json_log  '{"@timestamp":"$time_iso8601",'
       '"host":"$hostname",'
       '"server_ip":"$server_addr",'
       '"client_ip":"$remote_addr",'
       '"xff":"$http_x_forwarded_for",'
       '"domain":"$host",'
       '"url":"$uri",'
       '"referer":"$http_referer",'
       '"args":"$args",'
       '"upstreamtime":"$upstream_response_time",'
       '"responsetime":"$request_time",'
       '"request_method":"$request_method",'
       '"status":"$status",'
       '"size":"$body_bytes_sent",'
       '"request_length":"$request_length",'
       '"protocol":"$server_protocol",'
       '"upstreamhost":"$upstream_addr",'
       '"file_dir":"$request_filename",'
       '"http_user_agent":"$http_user_agent"'
       '}';
   
       access_log  /var/log/nginx/access.log  main;
       sendfile        on;
       keepalive_timeout  1800;
       server_names_hash_bucket_size 512;
       client_body_buffer_size  50m;
       client_header_buffer_size 20m;
       client_max_body_size 100m;
       large_client_header_buffers 2 20m;
       client_body_timeout   3m;
       client_header_timeout 3m;
       send_timeout          3m;
       fastcgi_buffers 8 128k;
       fastcgi_buffer_size 128K;
       proxy_buffer_size 128k;
       proxy_buffers 16 32k;
       proxy_busy_buffers_size 128k;
   
       include /etc/nginx/conf.d/*.conf;
   }
   EOF
   ```

2. **业务配置文件（default.conf）**

   ```bash
   cat > /opt/nginx/conf.d/default.conf <<EOF
   # 后端网关 upstream
   upstream vhr-gateway {
       server 10.172.131.164:9999;
       server 10.172.131.98:9999;
   }
   
   # 前端基础服务 upstream
   upstream vhr-pc-base {
       server 10.172.131.214:80;
       server 10.172.131.58:80;
   }
   
   # 各前端模块 upstream
   upstream vhr-fe-flow { server 10.172.131.164:8081; server 10.172.131.98:8081; }
   upstream vhr-fe-perm { server 10.172.131.164:8082; server 10.172.131.98:8082; }
   upstream vhr-fe-res { server 10.172.131.164:8083; server 10.172.131.98:8083; }
   upstream vhr-fe-rule { server 10.172.131.164:8084; server 10.172.131.98:8084; }
   upstream vhr-fe-sys { server 10.172.131.164:8085; server 10.172.131.98:8085; }
   upstream hrzx-comp-sv-service { server 10.172.131.164:8086; server 10.172.131.98:8086; }
   
   # 服务配置
   server {
       listen       80;
       server_name localhost zhvhr.ciicsh.com;
       access_log /var/log/nginx/zhvhr.ciicsh.com.log     access_json_log;
       error_log /var/log/nginx/zhvhr.ciicsh.com.log       error;
       ssl_protocols   TLSv1.2;
   
       # 基础路径转发（前端基础服务）
       location / {
           proxy_pass http://vhr-pc-base/;
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
       }
   
       # 各前端模块路径转发
       location ~ ^/mfe-flow/(.*)$ { proxy_pass http://vhr-fe-flow/$1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }
       location ~ ^/mfe-perm/(.*)$ { proxy_pass http://vhr-fe-perm/$1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }
       location ~* ^/mfe-res/(.*)$ { proxy_pass http://vhr-fe-res/$1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }
       location ~ ^/mfe-rule/(.*)$ { proxy_pass http://vhr-fe-rule/$1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }
       location ~ ^/mfe-sys/(.*)$ { proxy_pass http://vhr-fe-sys/$1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }
   
       # 后端API转发
       location ~ ^/api/(.*)$ { proxy_pass http://vhr-gateway/$1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }
   
       # 监管模块转发
       location ~ ^/mfe-supervise/(.*)$ { proxy_pass http://hrzx-comp-sv-service/$1; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; }
   }
   EOF
   ```

3. **docker-compose.yml**

   ```bash
   cat > /opt/nginx/docker-compose.yml << EOF
   version: '3.8'
   services:
     nginx:
       image: harbor.ciicsh.com/library/nginx:1.22.1-alpine
       container_name: nginx
       privileged: true
       network_mode: host
       restart: always
       volumes:
         - /opt/nginx/nginx.conf:/etc/nginx/nginx.conf
         - /opt/nginx/conf.d:/etc/nginx/conf.d
         - /opt/nginx/logs:/var/log/nginx
       environment:
         - TZ=Asia/Shanghai
   EOF
   ```

##### 3. 启动与验证

1. **导入镜像**

   ```bash
   docker load < /opt/nginx/nginx_1.22.1-alpine-x86.tar.gz
   ```

2. **启动服务**

   ```bash
   cd /opt/nginx
   docker-compose -f docker-compose.yml up -d
   ```

3. **查看服务状态**

   ```bash
   docker-compose ps -a
   ```

#### 3.2.6 Keepalived 安装部署（高可用）

##### 1. 准备操作（基于麒麟系统本地 YUM 源）

1. **创建 ISO 挂载目录**

   ```bash
   mkdir -p /opt/iso /opt/kylin
   chmod 0755 /opt/iso /opt/kylin
   ```

2. **挂载麒麟 ISO 镜像**

   ```bash
   mount -t iso9660 -o ro,loop /opt/iso/Kylin-Server-V10-SP3-2403-Release-20240426-x86_64.iso /opt/kylin
   ```

3. **配置本地 YUM 源**

   ```bash
   cat > /etc/yum.repos.d/kylin-media.repo << EOF
   [kylin-media]
   name=Kylin Server Local Media
   baseurl=file:///opt/kylin
   enabled=1
   gpgcheck=0
   EOF
   ```

4. **刷新 YUM 缓存**

   ```bash
   yum clean all
   yum makecache
   ```

5. **安装 Keepalived**

   ```bash
   yum -y install keepalived
   ```

##### 2. 配置文件编写（主从节点区分）

1. **主节点配置（10.172.131.66）**

   ```bash
   cat > /etc/keepalived/keepalived.conf <<EOF
   global_defs {
       router_id RABBITMQ_CLUSTER
       script_user root
       enable_script_security
   }
   
   # Nginx健康检查脚本
   vrrp_script chk_nginx {
     script "/etc/keepalived/check_nginx.sh"
     interval 30
     weight -31
   }
   
   vrrp_instance VI_1 {
       state MASTER
       interface enp4s3  # 替换为实际网卡名（如eth0）
       virtual_router_id 12
       priority 100
       advert_int 1
       # 认证配置
       authentication {
           auth_type PASS
           auth_pass 1111
       }
       # 关联健康检查脚本
       track_script {
           chk_nginx
       }
       # 虚拟IP（VIP）
       virtual_ipaddress {
           10.172.133.184/24
       }
       # 单播配置（主节点IP）
       unicast_src_ip 10.172.131.66
       # 从节点IP列表
       unicast_peer {
           10.172.131.174
       }
   }
   EOF
   ```

2. **从节点配置（10.172.131.174）**

   ```bash
   cat > /etc/keepalived/keepalived.conf <<EOF
   global_defs {
       router_id RABBITMQ_CLUSTER
       script_user root
       enable_script_security
   }
   
   # Nginx健康检查脚本（同主节点）
   vrrp_script chk_nginx {
     script "/etc/keepalived/check_nginx.sh"
     interval 30
     weight -31
   }
   
   vrrp_instance VI_1 {
       state BACKUP  # 从节点设为BACKUP
       interface enp4s3  # 替换为实际网卡名
       virtual_router_id 12  # 与主节点一致
       priority 100  # 优先级与主节点一致（或略低，如90）
       advert_int 1
       # 认证配置（与主节点一致）
       authentication {
           auth_type PASS
           auth_pass 1111
       }
       # 关联健康检查脚本
       track_script {
           chk_nginx
       }
       # 虚拟IP（与主节点一致）
       virtual_ipaddress {
           10.172.133.184/24
       }
       # 单播配置（从节点IP）
       unicast_src_ip 10.172.131.174
       # 主节点IP列表
       unicast_peer {
           10.172.131.66
       }
   }
   EOF
   ```

3. **健康检查脚本（check_nginx.sh）**

   ```bash
   cat > /etc/keepalived/check_nginx.sh <<EOF
   #!/bin/bash
   # Nginx容器健康检查脚本
   NGINX_CONTAINER="nginx"  # 替换为实际Nginx容器名称
   
   # 检查Nginx配置语法
   docker exec "$NGINX_CONTAINER" nginx -t >/dev/null 2>&1
   exit_status=$?
   
   if [ $exit_status -eq 0 ]; then
       exit 0  # 服务正常
   else
       exit 1  # 服务异常
   fi
   EOF
   
   # 添加执行权限
   chmod +x /etc/keepalived/check_nginx.sh
   ```

##### 3. 启动与验证

1. **启动 Keepalived 服务（主从节点均执行）**

   ```bash
   systemctl start keepalived
   ```

2. **设置开机自启**

   ```bash
   systemctl enable keepalived
   ```

3. **查看服务状态**

   ```bash
   systemctl status keepalived
   ```

4. **验证虚拟 IP（主节点执行）**

   ```bash
   ip addr  # 查看是否存在10.172.133.184
   ```

#### 3.2.7 MinIO 安装部署（双节点复制）

##### 1. 准备操作（所有节点执行）

1. **上传文件**
   将 `mc.RELEASE.2023-02-28T00-12-59Z`、`minio.RELEASE.2023-02-27T18-10-45Z-linux-amd64` 上传至 `/opt/minio/` 目录。

2. **创建用户组与用户**

   ```bash
   groupadd -r minio-user
   useradd -M -r -g minio-user minio-user
   ```

3. **创建数据目录并赋权**

   ```bash
   mkdir -p /opt/minio/data
   chown -R minio-user:minio-user /opt/minio/
   ```

4. **移动文件到系统目录并赋权**

   ```bash
   cp /opt/minio/minio.RELEASE.2023-02-27T18-10-45Z-linux-amd64 /usr/local/bin/minio
   cp /opt/minio/mc.RELEASE.2023-02-28T00-12-59Z-linux-amd64 /usr/local/bin/mc
   chmod +x /usr/local/bin/minio /usr/local/bin/mc
   ```

##### 2. 配置系统服务（每个节点执行）

1. **创建 systemd 服务文件**

   ```bash
   cat > /etc/systemd/system/minio.service <<EOF
   [Unit]
   Description=MinIO
   Documentation=https://min.io/docs/minio/linux/index.html
   Wants=network-online.target
   After=network-online.target
   AssertFileIsExecutable=/usr/local/bin/minio
   
   [Service]
   WorkingDirectory=/usr/local
   User=minio-user
   Group=minio-user
   ProtectProc=invisible
   
   EnvironmentFile=-/etc/default/minio
   ExecStartPre=/bin/bash -c "if [ -z \"${MINIO_VOLUMES}\" ]; then echo \"Variable MINIO_VOLUMES not set in /etc/default/minio\"; exit 1; fi"
   ExecStart=/usr/local/bin/minio server $MINIO_OPTS $MINIO_VOLUMES
   
   Restart=always
   LimitNOFILE=65536
   TasksMax=infinity
   TimeoutStopSec=infinity
   SendSIGKILL=no
   
   [Install]
   WantedBy=multi-user.target
   EOF
   ```

2. **创建环境变量文件（区分节点）**

   - **节点 1（10.172.131.76）**：

     ```bash
     cat > /etc/default/minio <<EOF
     MINIO_ROOT_USER=admin
     MINIO_ROOT_PASSWORD=VHRminio@admin
     MINIO_VOLUMES="/opt/minio/data"
     MINIO_OPTS="--console-address :9001 --address 10.172.131.76:9000"
     MINIO_SERVER_URL="http://10.172.131.76:9000"
     EOF
     ```

   - **节点 2（10.172.131.24）**：

     ```bash
     cat > /etc/default/minio <<EOF
     MINIO_ROOT_USER=admin
     MINIO_ROOT_PASSWORD=VHRminio@admin
     MINIO_VOLUMES="/opt/minio/data"
     MINIO_OPTS="--console-address :9001 --address 10.172.131.24:9000"
     MINIO_SERVER_URL="http://10.172.131.24:9000"
     EOF
     ```

##### 3. 启动与配置站点复制

1. **启动 MinIO 服务（所有节点执行）**

   ```bash
   systemctl daemon-reload
   systemctl start minio.service
   systemctl enable minio.service
   # 查看状态
   systemctl status minio.service
   ```

2. **配置 MinIO 客户端别名（任一节点执行）**

   ```bash
   # 配置节点1别名
   mc alias set minio1 http://10.172.131.76:9000 admin VHRminio@admin
   # 配置节点2别名
   mc alias set minio2 http://10.172.131.24:9000 admin VHRminio@admin
   ```

3. **配置站点复制（主从同步）**

   ```bash
   # 从minio1复制到minio2
   mc admin replicate add minio1 minio2
   ```

4. **验证复制状态**

   ```bash
   # 查看站点信息
   mc admin replicate info minio1
   # 查看复制状态
   mc admin replicate status minio1
   ```

#### 3.2.8 Fanruan（FineReport）安装部署

##### 1. 环境准备

1. **检查 Docker 状态（确保未安装）**

   ```bash
   docker version  # 若已安装，需先卸载（参考前文Docker卸载步骤）
   ```

2. **防火墙配置**

   ```bash
   # 查看状态
   systemctl status firewalld
   # 如需开启，后续需重启Docker服务
   ```

3. **时区设置**

   ```bash
   # 查看当前时区
   timedatectl
   # 设置时区为上海
   timedatectl set-timezone Asia/Shanghai
   ```

4. **创建挂载目录并赋权**

   ```bash
   mkdir -p /opt/fanruan/data
   chmod -R 777 /opt/fanruan
   # 清空ACL规则
   setfacl -b /opt/fanruan/
   ```

5. **关闭 SELinux**

   ```bash
   # 临时关闭
   setenforce 0
   # 永久关闭
   vim /etc/selinux/config
   # 修改为：SELINUX=disabled
   ```

##### 2. 安装部署

1. **上传安装包**
   将 `finekey-operation-all.tar.gz` 上传至 `/opt/fanruan/` 目录。

2. **解压安装包**

   ```bash
   cd /opt/fanruan
   tar zxvf finekey-operation-all.tar.gz
   ```

3. **配置 finekey.yaml**

   ```bash
   cat > /opt/fanruan/finekey/conf/finekey.yaml <<EOF
   node:
     ip: 10.172.131.236       # 节点IP（运维平台节点）
     port: 22                 # SSH端口
     user:                    # SSH用户名（如root）
     password:                # SSH密码（无英文单引号）
   dataRootPath: /opt/fanruan/data
   repo:
     port: 5000               # 仓库端口
     url:                     # 已有仓库URL（可选）
     username:                # 仓库用户名（可选）
     password:                # 仓库密码（可选）
     ssl: true                # 是否启用SSL
   online: false              # 离线安装
   language: CN               # 语言（CN/TW/EN/KR/JA/MY）
   EOF
   ```

4. **执行安装**

   ```bash
   cd /opt/fanruan/finekey/bin
   ./finekey
   ```

##### 3. 访问与配置

1. **访问运维平台**
   浏览器打开：`http://10.172.131.236/ops/decision`
   首次访问需设置管理员账号密码。
2. **添加管理用户**
   1. 访问「运维平台管理 > 用户管理 > 所有用户」
   2. 点击「添加用户」创建业务用户
   3. 重启运维平台（首次配置后需重启以生成权限配置）。
3. **部署 FineReport 项目**
   在运维平台中按指引部署 FineReport 报表项目（参考 Fanruan 官方文档）。

## 四、关键访问信息

| 环境 | 服务          | 访问地址                             | 账号                                 | 密码              |
| ---- | ------------- | ------------------------------------ | ------------------------------------ | ----------------- |
| 生产 | MinIO         | 10.172.131.76:9001                   | admin                                | VHRminio@admin    |
| 生产 | RabbitMQ      | 10.172.131.111:15672                 | vhr                                  | VHRrabbitmq@admin |
| 生产 | Redis         | 10.172.131.145:6379                  | -                                    | VHRredis@admin    |
| 生产 | Elasticsearch | 10.172.131.206:9200                  | elastic                              | VHRelastic@admin  |
| 生产 | Kibana        | 10.172.131.206:5601                  | kibana_system                        | VHRkibana@admin   |
| 生产 | DM 数据库     | 10.172.131.156:5236                  | CSP_ZH/CSP_ZH_ACTIVITI/CSP_ZH_XXLJOB | dmDBzdcXQvk!F87H  |
| 生产 | 运维平台      | 10.172.131.236:80/ops/decision       | admin                                | VHRreport8@admin  |
| 生产 | FineReport    | 10.172.131.227:8080/webroot/decision | admin                                | VHRreport8@admin  |
| 测试 | MinIO         | 10.172.131.73:9001                   | minioadmin                           | minioadmin        |
| 测试 | RabbitMQ      | 10.172.131.73:15672                  | admin                                | AAAaaa1234        |
| 测试 | Redis         | 10.172.131.73:6379                   | -                                    | AAAaaa1234        |
| 测试 | Elasticsearch | 10.172.131.73:9200                   | elastic                              | AAAaaa1234        |
| 测试 | Kibana        | 10.172.131.73:5601                   | kibana_system                        | AAAaaa1234        |
| 测试 | DM 数据库     | 10.172.131.73:5236                   | CSP_ZH/CSP_ZH_ACTIVITI/CSP_ZH_XXLJOB | dmDBzdcXQvk!F87H  |
| 测试 | 运维平台      | 10.172.131.197/ops/decision          | admin                                | VHRreport8@admin  |
| 测试 | FineReport    | 10.172.131.197:8080/webroot/decision | admin                                | VHRreport8@admin  |